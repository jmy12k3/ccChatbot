[strings]
log_dir = log_dir
model_data = model_data

resource_data = train_data/xiaohuangji50w.conv
seq_data = train_data/seq.data

input_vocab_path = train_data/inp.vocab
target_vocab_path = train_data/tar.vocab

e = E
m = M

[ints]
# 'Attention is All You Need': 6, 512, 2048, 8
# https://arxiv.org/abs/1706.03762
# BERT: 12, 768, 768, 12
num_layers = 12
d_model = 768
dff = 768
num_heads = 12

epochs = 10
batch_size = 64

max_length = 25
input_vocab_size = 10000
target_vocab_size = 10000

[floats]
dropout_rate = 0.1