[strings]
# Preprocessing path
resource_path = data/data.conv
seq_path = data/seq.data

# Model directories
log_dir = logs
model_dir = model

[ints]
# Hyperparameters
num_layers = 4
d_model = 128
dff = 512
num_heads = 8

# Tokenizer related
max_length = 20
vocab_size = 10000

# Training related
batch_size = 64
epoch = 20

[floats]
# Hyperparameter
dropout_rate = 0.1